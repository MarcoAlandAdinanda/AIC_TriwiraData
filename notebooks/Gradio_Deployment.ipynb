{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deployment on Gradio-Colab\n",
        "> **Note:** Due to limitations of GPU resources, the deployment uses `Google Colab T4 GPU` to load and runs LLM and embedding models.\n",
        "\n",
        "This notebook will runs gradio application and will make a public link."
      ],
      "metadata": {
        "id": "mZxUfEiqZbWZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Get setup\n",
        "Let's start by downloading all the modules needed."
      ],
      "metadata": {
        "id": "H0JkmipEa1dW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading modules and get setup"
      ],
      "metadata": {
        "id": "dRhnD58gbKdd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!wget https://raw.githubusercontent.com/MarcoAlandAdinanda/AIC_TriwiraData/main/deployments/setup.sh\n",
        "!sudo chmod +x setup.sh\n",
        "!./setup.sh\n",
        "!ollama pull MarcoAland/llama3.1-rag-indo"
      ],
      "metadata": {
        "id": "K0MuTHrnPyQP"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Start gradio app\n",
        "This code will automatically generate the gradio app inside the colab. To run the app outside colab, you can access the `public link` on any devide."
      ],
      "metadata": {
        "id": "jDkqzfT8c8Eh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAG module code:"
      ],
      "metadata": {
        "id": "fYkWFg9pa6jP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python app.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xuetmT3diz_",
        "outputId": "108fb7b9-89ed-4415-c04b-e8caccbaaae9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-09-04 05:22:37.190197: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-09-04 05:22:37.443133: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-09-04 05:22:37.512993: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-09-04 05:22:37.910857: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-09-04 05:22:40.040358: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "LLM is explicitly disabled. Using MockLLM.\n",
            "modules.json: 100% 349/349 [00:00<00:00, 2.32MB/s]\n",
            "config_sentence_transformers.json: 100% 201/201 [00:00<00:00, 1.30MB/s]\n",
            "README.md: 100% 16.9k/16.9k [00:00<00:00, 60.3MB/s]\n",
            "sentence_bert_config.json: 100% 54.0/54.0 [00:00<00:00, 420kB/s]\n",
            "config.json: 100% 698/698 [00:00<00:00, 3.74MB/s]\n",
            "model.safetensors: 100% 2.27G/2.27G [01:35<00:00, 23.7MB/s]\n",
            "tokenizer_config.json: 100% 1.17k/1.17k [00:00<00:00, 5.86MB/s]\n",
            "sentencepiece.bpe.model: 100% 5.07M/5.07M [00:00<00:00, 6.26MB/s]\n",
            "tokenizer.json: 100% 17.1M/17.1M [00:00<00:00, 28.4MB/s]\n",
            "special_tokens_map.json: 100% 964/964 [00:00<00:00, 7.50MB/s]\n",
            "1_Pooling/config.json: 100% 297/297 [00:00<00:00, 1.88MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/utils.py:1002: UserWarning: Expected 2 arguments for function <function chat at 0x7e77d019ff40>, received 1.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/utils.py:1006: UserWarning: Expected at least 2 arguments for function <function chat at 0x7e77d019ff40>, received 1.\n",
            "  warnings.warn(\n",
            "Running on local URL:  http://127.0.0.1:7860\n",
            "Running on public URL: https://66963315f1339f4a96.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/helpers.py:978: UserWarning: Unexpected argument. Filling with None.\n",
            "  warnings.warn(\"Unexpected argument. Filling with None.\")\n"
          ]
        }
      ]
    }
  ]
}